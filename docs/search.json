[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tayyip Sinan Ocaktan’s Progress Journal",
    "section": "",
    "text": "This progress journal covers Tayyip Sinan Ocaktan’s work during their term at BDA 503 Fall 2022.\nEach section is an assignment or an individual work."
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "1  Assignment 1",
    "section": "",
    "text": "Greetings! My name is Sinan. I have a BSc. in geomatics engineering from ITU, and I have been working as a data scientist for over a year now. I stumbled into machine learning when I was trying to calculate a missing part of some quantitative data for a school project of mine. I still remember what I googled: “create an equation with known x and ys”. Turns out my super original idea was already there for decades! I fell in love with the field and the community of it, so I am here! Find out more about me on my LinkedIn page."
  },
  {
    "objectID": "assignment1.html#user-2022",
    "href": "assignment1.html#user-2022",
    "title": "1  Assignment 1",
    "section": "1.2 UseR-2022",
    "text": "1.2 UseR-2022\n\nEmil Hvitfeldt - Improvements in text preprocessing using `textrecipes`\nThe speaker promotes the newest improvements in textrecipes library. textrecipes is a library that helps with text processing for NLP. The speech consists of 3 main capabilities of the library: tokenization, modification and numbers.\n\nTokenization\n\nUsing raw text is not a great way to talk with computers. One of the main processes in text processing is using tokens. We take pieces of large strings and store them in tokens to extract information in a more specific way and help computers understand human language. There are several tokenization methods in textrecipes; one can split texts into characters, words, sentences and bytes.\n\n\nModification\n\nWe also need to modify our texts to avoid misleading parts of them and get more explanatory information.\nOne of the modification methods is stemming, which is a process to access the root meaning of the words. Despite the differences in their appearance, some words can contain the same meaning, and we want them to describe the same thing to a computer.\nAnother method is to remove the stopwords. There are some words such as “and”, “the” can be repeated a lot in texts while not meaning a lot. They are usually dropped in NLP projects.\nSometimes computers store the same looking characters in different ways, and while a human can see the same thing, the 2 characters will mean 2 different things to a computer. This happens in characters such as “İ”, “ö” etc. textrecipes get through these difficulties by text normalization.\nWhile a word may mean a thing, its’ order and the word before or after that can be important. One can use sentence tokenization to capture them, but it has its own disadvantages. textrecipes uses n-grams to store tokens in pairs and get their meanings without needing sentences.\n\n\nNumbers\n\nThere is one important issue left with our texts: they are still texts! We want to transform our tokens into numbers so they will have numbers to describe themselves. We can count our tokens with step_tf feature of textrecipes. Great, now we know how many times they appeared in our set, yet we may want to transform them into frequencies as well. We call this way of measure as “tf-idf”. textrecipes can also help us with that."
  },
  {
    "objectID": "assignment1.html#r-posts",
    "href": "assignment1.html#r-posts",
    "title": "1  Assignment 1",
    "section": "1.3 R Posts",
    "text": "1.3 R Posts\n\nKable\nWe talked about text a lot, yet I’d like to go on with them. Here is a fun dataset which consists of dialogues from the first Harry Potter book.\n\n\n  scene     character_name\n1     1   Albus Dumbledore\n2     1 Minerva McGonagall\n3     1   Albus Dumbledore\n4     1 Minerva McGonagall\n5     1   Albus Dumbledore\n6     1 Minerva McGonagall\n                                                                   dialogue\n1         I should have known that you would be here, Professor McGonagall.\n2           Good evening, Professor Dumbledore. Are the rumours true Albus?\n3                          I'm afraid so, Professor. The good, and the bad.\n4                                                              And the boy?\n5                                                   Hagrid is bringing him.\n6 Do you think it wise to trust Hagrid with something as important as this?\n\n\nWe can see our data set here, yet it is not easy on the eyes. That’s why I spared my first topic for “kable” function of “knitr” package. You can find out more about it in this link.\nTo summary, kable is a great tool to adjust the options for our tables. A simple code like below will change the whole appearance.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\nscene\ncharacter_name\ndialogue\n\n\n\n\n1\nAlbus Dumbledore\nI should have known that you would be here, Professor McGonagall.\n\n\n1\nMinerva McGonagall\nGood evening, Professor Dumbledore. Are the rumours true Albus?\n\n\n1\nAlbus Dumbledore\nI’m afraid so, Professor. The good, and the bad.\n\n\n1\nMinerva McGonagall\nAnd the boy?\n\n\n1\nAlbus Dumbledore\nHagrid is bringing him.\n\n\n1\nMinerva McGonagall\nDo you think it wise to trust Hagrid with something as important as this?\n\n\n\n\n\nBut what about if we like to change more. Column names can change and we can allign values in the columns for a neater look.\n\nkable(head(df), col.names = c(\"Scene Number\", \"Character\", \"The Dialogue\"), align = \"clr\")\n\n\n\n\n\n\n\n\n\nScene Number\nCharacter\nThe Dialogue\n\n\n\n\n1\nAlbus Dumbledore\nI should have known that you would be here, Professor McGonagall.\n\n\n1\nMinerva McGonagall\nGood evening, Professor Dumbledore. Are the rumours true Albus?\n\n\n1\nAlbus Dumbledore\nI’m afraid so, Professor. The good, and the bad.\n\n\n1\nMinerva McGonagall\nAnd the boy?\n\n\n1\nAlbus Dumbledore\nHagrid is bringing him.\n\n\n1\nMinerva McGonagall\nDo you think it wise to trust Hagrid with something as important as this?\n\n\n\n\n\nThere are more things that can be done with kable and similar packages but we got what we need for now.\n\n\nCounts\nWe stored our data in a data frame, took a look to our data and can work with it now. I would like to count the appearences of the characters in the book and working with a data frame helps us a lot to do so. This link gives cool tricks for this task.\n\ndf_freq <- aggregate(df$character_name, by=list(df$character_name), FUN=length)\ndf_freq <- df_freq[order(-df_freq$x),]\nkable(head(df_freq), row.names = FALSE, col.names = c(\"Character\", \"Dialogue Count\"), align = \"lc\")\n\n\n\n\nCharacter\nDialogue Count\n\n\n\n\nHarry Potter\n230\n\n\nRon Weasley\n120\n\n\nHermione Granger\n92\n\n\nRubeus Hagrid\n81\n\n\nMinerva McGonagall\n31\n\n\nAlbus Dumbledore\n24\n\n\n\n\n\nNot so surprisingly, Harry Potter has the most dialogue with almost twice count of his follower: Ron Weasley.\n\n\nWord Cloud\nFinally, we will create a word cloud. It is not exactly a word cloud rather a character cloud. Word clouds are used to visualize text and they look great in my opinion. We use wordcloud2 package to visualize these characters. There is not an easy way to show this cloud on HTMLs, so we save it locally then add it to the page.\n\nlibrary(wordcloud2)\nlibrary(webshot)\nlibrary(htmlwidgets)\ncloud=wordcloud2(data=df_freq, size=0.75)\nsaveWidget(cloud, \"words.html\", selfcontained = F)\nwebshot(\"words.html\", \"words.png\", delay = 6, vwidth = 1000, vheight = 1000)\n\n\n\n\nThere! We have a nice visualization on characters which we can see appearances of relatively."
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "2  Assignment 2",
    "section": "",
    "text": "Here we can take a look at our dataset."
  },
  {
    "objectID": "assignment2.html#about-me",
    "href": "assignment2.html#about-me",
    "title": "2  Assignment 2",
    "section": "2.2 About Me",
    "text": "2.2 About Me\nGreetings! My name is Sinan. I have a BSc. in geomatics engineering from ITU, and I have been working as a data scientist for over a year now. I stumbled into machine learning when I was trying to calculate a missing part of some quantitative data for a school project of mine. I still remember what I googled: “create an equation with known x and ys”. Turns out my super original idea was already there for decades! I fell in love with the field and the community of it, so I am here! Find out more about me on my LinkedIn page."
  },
  {
    "objectID": "assignment2.html#user-2022",
    "href": "assignment2.html#user-2022",
    "title": "2  Assignment 2",
    "section": "2.3 UseR-2022",
    "text": "2.3 UseR-2022\n\nEmil Hvitfeldt - Improvements in text preprocessing using `textrecipes`\nThe speaker promotes the newest improvements in textrecipes library. textrecipes is a library that helps with text processing for NLP. The speech consists of 3 main capabilities of the library: tokenization, modification and numbers.\n\nTokenization\n\nUsing raw text is not a great way to talk with computers. One of the main processes in text processing is using tokens. We take pieces of large strings and store them in tokens to extract information in a more specific way and help computers understand human language. There are several tokenization methods in textrecipes; one can split texts into characters, words, sentences and bytes.\n\n\nModification\n\nWe also need to modify our texts to avoid misleading parts of them and get more explanatory information.\nOne of the modification methods is stemming, which is a process to access the root meaning of the words. Despite the differences in their appearance, some words can contain the same meaning, and we want them to describe the same thing to a computer.\nAnother method is to remove the stopwords. There are some words such as “and”, “the” can be repeated a lot in texts while not meaning a lot. They are usually dropped in NLP projects.\nSometimes computers store the same looking characters in different ways, and while a human can see the same thing, the 2 characters will mean 2 different things to a computer. This happens in characters such as “İ”, “ö” etc. textrecipes get through these difficulties by text normalization.\nWhile a word may mean a thing, its’ order and the word before or after that can be important. One can use sentence tokenization to capture them, but it has its own disadvantages. textrecipes uses n-grams to store tokens in pairs and get their meanings without needing sentences.\n\n\nNumbers\n\nThere is one important issue left with our texts: they are still texts! We want to transform our tokens into numbers so they will have numbers to describe themselves. We can count our tokens with step_tf feature of textrecipes. Great, now we know how many times they appeared in our set, yet we may want to transform them into frequencies as well. We call this way of measure as “tf-idf”. textrecipes can also help us with that."
  },
  {
    "objectID": "assignment2.html#r-posts",
    "href": "assignment2.html#r-posts",
    "title": "2  Assignment 2",
    "section": "2.4 R Posts",
    "text": "2.4 R Posts\n\nKable\nWe talked about text a lot, yet I’d like to go on with them. Here is a fun dataset which consists of dialogues from the first Harry Potter book.\n\n\n  scene     character_name\n1     1   Albus Dumbledore\n2     1 Minerva McGonagall\n3     1   Albus Dumbledore\n4     1 Minerva McGonagall\n5     1   Albus Dumbledore\n6     1 Minerva McGonagall\n                                                                   dialogue\n1         I should have known that you would be here, Professor McGonagall.\n2           Good evening, Professor Dumbledore. Are the rumours true Albus?\n3                          I'm afraid so, Professor. The good, and the bad.\n4                                                              And the boy?\n5                                                   Hagrid is bringing him.\n6 Do you think it wise to trust Hagrid with something as important as this?\n\n\nWe can see our data set here, yet it is not easy on the eyes. That’s why I spared my first topic for “kable” function of “knitr” package. You can find out more about it in this link.\nTo summary, kable is a great tool to adjust the options for our tables. A simple code like below will change the whole appearance.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\nscene\ncharacter_name\ndialogue\n\n\n\n\n1\nAlbus Dumbledore\nI should have known that you would be here, Professor McGonagall.\n\n\n1\nMinerva McGonagall\nGood evening, Professor Dumbledore. Are the rumours true Albus?\n\n\n1\nAlbus Dumbledore\nI’m afraid so, Professor. The good, and the bad.\n\n\n1\nMinerva McGonagall\nAnd the boy?\n\n\n1\nAlbus Dumbledore\nHagrid is bringing him.\n\n\n1\nMinerva McGonagall\nDo you think it wise to trust Hagrid with something as important as this?\n\n\n\n\n\nBut what about if we like to change more. Column names can change and we can allign values in the columns for a neater look.\n\nkable(head(df), col.names = c(\"Scene Number\", \"Character\", \"The Dialogue\"), align = \"clr\")\n\n\n\n\n\n\n\n\n\nScene Number\nCharacter\nThe Dialogue\n\n\n\n\n1\nAlbus Dumbledore\nI should have known that you would be here, Professor McGonagall.\n\n\n1\nMinerva McGonagall\nGood evening, Professor Dumbledore. Are the rumours true Albus?\n\n\n1\nAlbus Dumbledore\nI’m afraid so, Professor. The good, and the bad.\n\n\n1\nMinerva McGonagall\nAnd the boy?\n\n\n1\nAlbus Dumbledore\nHagrid is bringing him.\n\n\n1\nMinerva McGonagall\nDo you think it wise to trust Hagrid with something as important as this?\n\n\n\n\n\nThere are more things that can be done with kable and similar packages but we got what we need for now.\n\n\nCounts\nWe stored our data in a data frame, took a look to our data and can work with it now. I would like to count the appearences of the characters in the book and working with a data frame helps us a lot to do so. This link gives cool tricks for this task.\n\ndf_freq <- aggregate(df$character_name, by=list(df$character_name), FUN=length)\ndf_freq <- df_freq[order(-df_freq$x),]\nkable(head(df_freq), row.names = FALSE, col.names = c(\"Character\", \"Dialogue Count\"), align = \"lc\")\n\n\n\n\nCharacter\nDialogue Count\n\n\n\n\nHarry Potter\n230\n\n\nRon Weasley\n120\n\n\nHermione Granger\n92\n\n\nRubeus Hagrid\n81\n\n\nMinerva McGonagall\n31\n\n\nAlbus Dumbledore\n24\n\n\n\n\n\nNot so surprisingly, Harry Potter has the most dialogue with almost twice count of his follower: Ron Weasley.\n\n\nWord Cloud\nFinally, we will create a word cloud. It is not exactly a word cloud rather a character cloud. Word clouds are used to visualize text and they look great in my opinion. We use wordcloud2 package to visualize these characters. There is not an easy way to show this cloud on HTMLs, so we save it locally then add it to the page.\n\nlibrary(wordcloud2)\nlibrary(webshot)\nlibrary(htmlwidgets)\ncloud=wordcloud2(data=df_freq, size=0.75)\nsaveWidget(cloud, \"words.html\", selfcontained = F)\nwebshot(\"words.html\", \"words.png\", delay = 6, vwidth = 1000, vheight = 1000)\n\n\n\n\nThere! We have a nice visualization on characters which we can see appearances of relatively."
  },
  {
    "objectID": "In-class assignment.html",
    "href": "In-class assignment.html",
    "title": "2  In-class assignment",
    "section": "",
    "text": "Here we can take a look at our dataset.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\nN10156\n2004\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNA\nTurbo-fan\n\n\nN102UW\n1998\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN103US\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN104UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN10575\n2002\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNA\nTurbo-fan\n\n\nN105UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\n\n\n\n\n\n\n\nWe can calculate the mean seat number on flights changing by the year and visualize them on a plot. We filter out older than 1985 to get more stable results.\n\nseats <- df %>% group_by(year) %>% dplyr::summarize(Mean = mean(seats),) %>% filter(year > 1985)\nseats[order(seats$year),]\n\n# A tibble: 28 × 2\n    year  Mean\n   <int> <dbl>\n 1  1986  185.\n 2  1987  181.\n 3  1988  190.\n 4  1989  163.\n 5  1990  179.\n 6  1991  181.\n 7  1992  194.\n 8  1993  194.\n 9  1994  174.\n10  1995  187.\n# … with 18 more rows\n\n\n\n\n\n\nggplot(data=seats,aes(x=year,y=Mean)) +\n  geom_line()\n\n\n\n\nIn the early 2000s, mean seat numbers decreased significantly. Note that y-axis does not start from zero\n\n\n\n\n\n\nWe can\n\nman = df %>% count(manufacturer, year, sort = TRUE) %>% filter(n > 10)\nman\n\n# A tibble: 79 × 3\n   manufacturer      year     n\n   <chr>            <int> <int>\n 1 BOEING            2001   142\n 2 BOEING            2000   134\n 3 BOEING            1999   124\n 4 BOEING            1998   103\n 5 AIRBUS INDUSTRIE  2001    82\n 6 AIRBUS INDUSTRIE  2000    80\n 7 BOEING            2004    77\n 8 BOMBARDIER INC    2004    72\n 9 BOEING            2008    68\n10 BOEING            2006    66\n# … with 69 more rows\n\n\n\n\n\n\nggplot(data=man,aes(x=year,y=n,color=manufacturer)) +\n  geom_line()\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\n\n\n\nWe can see the popularity of the manufacturers changing by the year."
  }
]